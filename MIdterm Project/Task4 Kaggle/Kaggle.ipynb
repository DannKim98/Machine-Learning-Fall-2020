{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashon-MNIST\n",
    "You can find my notebook published on https://www.kaggle.com/dannkim/fashon-mnist-classification-from-scratch\n",
    "<br>Classification of Fashion MNIST data set, from scratch using only numpy. And comparison of the implemented model with sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "- Download the data set from https://www.kaggle.com/zalando-research/fashionmnist\n",
    "- Load it using pandas\n",
    "- Store the pixels values into X and labels into Y\n",
    "- Normalize the data dividing by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./data/fashion-mnist_train.csv')\n",
    "data_test = pd.read_csv('./data/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.values[:, 1:]\n",
    "Y_train = data_train.values[:, 0]\n",
    "X_test = data_test.values[:, 1:]\n",
    "Y_test = data_test.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.astype('float32'), X_test.astype('float32'), \n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log_regression(X, Y, T, learning_rat)\n",
    "- The log_regression() function optimizes the weights by minimizing a Cross Entropy Error using stochastic gradient descent\n",
    "- X is a training data set\n",
    "- Y is corresponding labels\n",
    "- T is a number of iterations (default is 50000)\n",
    "- learning_rate is a learning rate of the model (default is initially 0.01 and decreasing during the training\n",
    "- Final weights are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_regression(X, Y, T=50000, learning_rate=0.01):\n",
    "    N = X.shape[0]\n",
    "    W = np.zeros((X.shape[1],)) \n",
    "    for i in range(T):\n",
    "        if i == 20000:\n",
    "            learning_rate = 0.005\n",
    "        elif i == 30000:\n",
    "            learning_rate = 0.001\n",
    "        rand_index = np.random.choice(N, size=1)\n",
    "        x_n = X[rand_index][0]\n",
    "        E_dev = -1 * (Y[rand_index] * x_n)/(1 + np.exp((Y[rand_index][0] * np.dot(x_n, W))))\n",
    "        W = W - learning_rate * E_dev\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_D_Y(Y, k)\n",
    "- The model uses One vs All approach for multicalss classificattion \n",
    "- The generate_D_Y() function generates new labels for each class\n",
    "- Y is an original set labels\n",
    "- k is the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_D_Y(Y, k):\n",
    "    Y_copy = np.copy(Y)\n",
    "    for i, y in enumerate(Y_copy):\n",
    "        Y_copy[i] = 2 * (int(y) == k) - 1\n",
    "    return Y_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid(x)\n",
    "- The sigmoid function is used to predict the probability that x belongs to the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate prob(W, X)\n",
    "- The calculate_prob() function calculate the probability of each example belongs to each class\n",
    "- W is the set of weights genetared by log_regression on each class k\n",
    "- X is a test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob(W, X):\n",
    "    probs = []\n",
    "    for i in range(10):\n",
    "        probs.append(sigmoid(np.dot(X, W[i])))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict(p)\n",
    "- The predict() function predicts the actual class by getting the most probable one\n",
    "- p is probabilities calculated by calculate_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(p):\n",
    "    Y_pred = []\n",
    "    for i in range(len(p[0])):\n",
    "        Y_pred.append(np.argmax(p[:,i]))\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_accuracy(Y_pred, Y)\n",
    "- The calculate-accuracy function evaluates the accuracy of the model by comparing predicted values Y_pred with actual labels Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(Y_pred, Y):\n",
    "    correct = 0\n",
    "    for i in range(len(Y)):\n",
    "        if Y_pred[i] == Y[i]:\n",
    "            correct += 1\n",
    "    return correct/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def fit(X, Y, k):\n",
    "        W = []\n",
    "        for i in range(k):\n",
    "            W.append(log_regression(X, generate_D_Y(Y, i)))\n",
    "        return W\n",
    "\n",
    "    def score(X, Y, W):\n",
    "        probs = calculate_prob(W, X)\n",
    "        Y_pred = predict(np.array(probs))\n",
    "        return calculate_accuracy(Y_pred, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Implemented Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = logistic_regression\n",
    "W_log = classifier.fit(X_train, Y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Implemented Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8323\n"
     ]
    }
   ],
   "source": [
    "print(classifier.score(X_test, Y_test, W_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='liblinear', multi_class='auto').fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the sklearn.linear_model.LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "It can be seen that the performance of my model is approximately the same as the performance of sklearn build-in model. Therefore it can be concluded that the implementation was done correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
